Input sentence  MachineLEarning engineers live in California
Input tokens  ['machine', '##lea', '##rn', '##ing', 'engineers', 'live', 'in', 'california']
Input tokens after classification and sentence pair  ['[CLS]', 'machine', '##lea', '##rn', '##ing', 'engineers', 'live', 'in', 'california', '[SEP]']
Len of tokens  10
Input tokens after padding  ['[CLS]', 'machine', '##lea', '##rn', '##ing', 'engineers', 'live', 'in', 'california', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']
attention_mask  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
token ids  [101, 3698, 19738, 6826, 2075, 6145, 2444, 1999, 2662, 102, 0, 0, 0, 0, 0, 0]
tensor([[  101,  3698, 19738,  6826,  2075,  6145,  2444,  1999,  2662,   102,
             0,     0,     0,     0,     0,     0]]) torch.Size([1, 16]) 2
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]) torch.Size([1, 16]) 2
model ouput  BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0287,  0.1711, -0.0128,  ..., -0.4202,  0.2021,  0.0992],
         [ 0.4537,  0.2695, -0.2365,  ..., -0.1391,  0.5434, -0.0229],
         [ 0.8245,  0.2420,  0.5221,  ...,  0.0926, -0.2089, -1.2964],
         ...,
         [-0.2026, -0.0621, -0.2736,  ...,  0.1625,  0.3685, -0.1595],
         [ 0.0400,  0.1936, -0.0795,  ...,  0.1241,  0.1658, -0.0950],
         [ 0.1787,  0.1993,  0.0787,  ...,  0.0352, -0.0963,  0.0263]]],
       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9201, -0.3985, -0.5720,  0.7975,  0.1429, -0.2849,  0.8703,  0.3495,
         -0.1327, -1.0000, -0.3231,  0.7449,  0.9840,  0.0402,  0.9247, -0.7152,
         -0.1399, -0.5538,  0.3735, -0.6741,  0.6617,  0.9992,  0.3397,  0.2989,
          0.4410,  0.8179, -0.6944,  0.9250,  0.9590,  0.7819, -0.7315,  0.3818,
         -0.9853, -0.2309, -0.3506, -0.9914,  0.3866, -0.7974,  0.0987, -0.2086,
         -0.8607,  0.5227,  0.9999, -0.2481,  0.1643, -0.3447, -1.0000,  0.3144,
         -0.9029,  0.4207,  0.0969,  0.1209,  0.2860,  0.4838,  0.5313, -0.1048,
         -0.0293,  0.1877, -0.2336, -0.6377, -0.6680,  0.3270, -0.4378, -0.9175,
          0.2531,  0.0276, -0.2237, -0.3239, -0.1095,  0.0433,  0.8993,  0.1853,
          0.1034, -0.7954, -0.1024,  0.3088, -0.4935,  1.0000, -0.5011, -0.9788,
          0.1536,  0.1102,  0.4099,  0.3137, -0.1867, -1.0000,  0.4424, -0.1483,
         -0.9890,  0.3466,  0.4489, -0.2139, -0.3190,  0.4476, -0.1223, -0.2874,
         -0.3470, -0.1432, -0.2533, -0.3867,  0.1843, -0.2451, -0.1570, -0.3800,
          0.1912, -0.5373, -0.4053,  0.2346, -0.1112,  0.6140,  0.3083, -0.3326,
          0.3305, -0.9546,  0.6820, -0.3623, -0.9831, -0.5645, -0.9883,  0.7725,
         -0.0606, -0.2560,  0.9674,  0.5506,  0.3883, -0.0829, -0.2468, -1.0000,
         -0.5666, -0.1913,  0.1152, -0.1803, -0.9804, -0.9561,  0.5963,  0.9529,
          0.2781,  0.9996, -0.3875,  0.9327,  0.1688, -0.3577, -0.1834, -0.5177,
          0.4661,  0.2718, -0.7664,  0.2610, -0.3218, -0.1469, -0.4953, -0.3372,
         -0.0187, -0.9242, -0.4680,  0.9350, -0.0490, -0.4376,  0.5362, -0.3367,
         -0.3995,  0.8502,  0.5654,  0.3640, -0.0533,  0.4434, -0.2963,  0.5749,
         -0.8383, -0.1091,  0.4616, -0.3521, -0.0307, -0.9780, -0.4317,  0.5142,
          0.9870,  0.8150,  0.3824,  0.4332, -0.3893,  0.5229, -0.9496,  0.9791,
         -0.1990,  0.1865,  0.3356, -0.1143, -0.8627, -0.4059,  0.8901, -0.5583,
         -0.8789, -0.1210, -0.5310, -0.5092, -0.3645,  0.6228, -0.3043, -0.4844,
         -0.1780,  0.9114,  0.9741,  0.7702, -0.3194,  0.5479, -0.9094, -0.4254,
          0.2555,  0.3766,  0.1481,  0.9932, -0.3823, -0.2619, -0.8759, -0.9814,
          0.0850, -0.8913, -0.1480, -0.7758,  0.5041,  0.0957,  0.0269,  0.3273,
         -0.9765, -0.7788,  0.3253, -0.4209,  0.4508, -0.3051,  0.5208,  0.5762,
         -0.6593,  0.6162,  0.9192,  0.0076, -0.7527,  0.8257, -0.3184,  0.9090,
         -0.6349,  0.9943,  0.5426,  0.6539, -0.9387, -0.2025, -0.9103, -0.1587,
         -0.0260, -0.4845,  0.3145,  0.4991,  0.4535,  0.7681, -0.6213,  0.9960,
         -0.4377, -0.9557, -0.2134, -0.1527, -0.9871,  0.2107,  0.3355, -0.1567,
         -0.4070, -0.5981, -0.9369,  0.9086,  0.2275,  0.9904, -0.1116, -0.9137,
         -0.4237, -0.9226,  0.0145, -0.1850,  0.4057, -0.0666, -0.9421,  0.4834,
          0.6097,  0.5696, -0.1741,  0.9970,  1.0000,  0.9737,  0.8916,  0.9059,
         -0.9949, -0.3433,  1.0000, -0.9106, -1.0000, -0.9464, -0.6987,  0.4487,
         -1.0000, -0.0852, -0.0870, -0.9234,  0.0377,  0.9796,  0.9852, -1.0000,
          0.9045,  0.9505, -0.5514,  0.6487, -0.3464,  0.9655,  0.1667,  0.5404,
         -0.3103,  0.3848, -0.5370, -0.8892,  0.0898,  0.0563,  0.9706,  0.1973,
         -0.7946, -0.9246,  0.3652, -0.1481, -0.2967, -0.9534, -0.2091,  0.1161,
          0.4934,  0.2424,  0.2369, -0.8084,  0.2965, -0.6796,  0.5022,  0.5758,
         -0.9447, -0.7544, -0.1019, -0.0822, -0.0767, -0.9571,  0.9737, -0.4322,
          0.4874,  1.0000,  0.1912, -0.9121,  0.3357,  0.2627,  0.1386,  1.0000,
          0.5915, -0.9840, -0.4831,  0.4882, -0.5363, -0.4910,  0.9994, -0.3331,
         -0.0957,  0.1828,  0.9762, -0.9898,  0.8655, -0.9049, -0.9582,  0.9576,
          0.9318, -0.1723, -0.7516,  0.1542, -0.1433,  0.2726, -0.9434,  0.6630,
          0.4395, -0.2326,  0.8740, -0.8724, -0.4397,  0.3685, -0.2394,  0.1021,
          0.3781,  0.4732, -0.2809,  0.1616, -0.2740, -0.1146, -0.9678,  0.0584,
          1.0000, -0.0469,  0.1535, -0.2972, -0.1382, -0.2156,  0.4347,  0.5423,
         -0.2913, -0.8327,  0.0377, -0.8876, -0.9887,  0.7494,  0.2508, -0.2950,
          1.0000,  0.4066,  0.3152,  0.0851,  0.8374, -0.0039,  0.5808,  0.0801,
          0.9707, -0.2899,  0.4567,  0.8503, -0.2566, -0.4320, -0.5864,  0.0917,
         -0.9147, -0.0602, -0.9603,  0.9567,  0.6640,  0.3816,  0.2012,  0.1523,
          1.0000, -0.4675,  0.6524, -0.4320,  0.7345, -0.9920, -0.7868, -0.3544,
         -0.0951, -0.1678, -0.3219,  0.1713, -0.9731,  0.1431,  0.1580, -0.9743,
         -0.9880,  0.1971,  0.8196,  0.1522, -0.8824, -0.6022, -0.6115,  0.5254,
         -0.1890, -0.9249,  0.4116, -0.3052,  0.5960, -0.3143,  0.4503,  0.0389,
          0.8588, -0.2347, -0.1927, -0.1683, -0.8224,  0.7815, -0.8574, -0.3605,
         -0.1285,  1.0000, -0.1324,  0.1535,  0.7739,  0.7451, -0.2995,  0.1502,
          0.3405,  0.3067,  0.0475, -0.2616, -0.6522, -0.4597,  0.6177, -0.0600,
         -0.1185,  0.7503,  0.5094,  0.3662, -0.0401,  0.1536,  0.9988, -0.2272,
         -0.2491, -0.7149, -0.1643, -0.3619, -0.4129,  1.0000,  0.3888,  0.3971,
         -0.9919, -0.2945, -0.9350,  1.0000,  0.8446, -0.8897,  0.5960,  0.4023,
         -0.2461,  0.7717, -0.3647, -0.3094,  0.3150,  0.1042,  0.9652, -0.5266,
         -0.9679, -0.7224,  0.4708, -0.9508,  0.9976, -0.5710, -0.3003, -0.3750,
         -0.0968,  0.6355,  0.0389, -0.9779, -0.1165,  0.0179,  0.9728,  0.2661,
         -0.4736, -0.9369,  0.2294,  0.2596, -0.2107, -0.9505,  0.9663, -0.9715,
          0.3834,  1.0000,  0.3603, -0.1350,  0.2732, -0.5026,  0.3133, -0.0660,
          0.5923, -0.9384, -0.2696, -0.2217,  0.4844, -0.2201, -0.3555,  0.7707,
          0.2599, -0.4014, -0.6768, -0.1532,  0.4882,  0.8140, -0.3622, -0.1207,
          0.1916, -0.0760, -0.9337, -0.4505, -0.3887, -0.9996,  0.7055, -1.0000,
         -0.0035, -0.3624, -0.2459,  0.8241,  0.3719,  0.2052, -0.7227, -0.1522,
          0.7057,  0.7538, -0.3984, -0.1717, -0.7394,  0.1841, -0.1118,  0.3218,
         -0.2541,  0.7514, -0.2808,  1.0000,  0.1824, -0.4547, -0.9634,  0.3533,
         -0.2808,  1.0000, -0.8606, -0.9605,  0.2766, -0.7001, -0.7964,  0.3469,
          0.0331, -0.8006, -0.6958,  0.9485,  0.8904, -0.4228,  0.4707, -0.3247,
         -0.5094,  0.0642,  0.4481,  0.9865,  0.3093,  0.9016,  0.1335, -0.0131,
          0.9613,  0.2407,  0.7145,  0.2795,  1.0000,  0.3701, -0.9269,  0.3266,
         -0.9786, -0.2728, -0.9462,  0.3527,  0.2682,  0.8810, -0.2849,  0.9633,
         -0.1895,  0.1004, -0.3089,  0.2481,  0.3346, -0.9287, -0.9858, -0.9844,
          0.5072, -0.5534, -0.0893,  0.3709,  0.1487,  0.5088,  0.4740, -1.0000,
          0.9476,  0.4181,  0.2072,  0.9705,  0.4569,  0.5397,  0.3840, -0.9869,
         -0.9665, -0.3514, -0.2477,  0.7819,  0.7027,  0.7930,  0.3847, -0.4731,
         -0.4028,  0.3816, -0.5748, -0.9914,  0.5021, -0.1028, -0.9605,  0.9452,
         -0.4928, -0.0909,  0.4067, -0.0444,  0.9642,  0.8695,  0.5324,  0.1592,
          0.6377,  0.9007,  0.9505,  0.9858, -0.4682,  0.8088, -0.3690,  0.3484,
          0.5853, -0.9405,  0.1713,  0.3636, -0.0499,  0.3397, -0.2456, -0.9503,
          0.5428, -0.2670,  0.5623, -0.4190,  0.1462, -0.4863, -0.1886, -0.7342,
         -0.5138,  0.5632,  0.3817,  0.9300,  0.4175, -0.2142, -0.6808, -0.3136,
          0.0127, -0.9168,  0.9223, -0.1826,  0.3780,  0.1609, -0.0648,  0.7638,
         -0.0291, -0.4544, -0.3379, -0.8128,  0.8693, -0.4001, -0.4783, -0.5516,
          0.7594,  0.3288,  0.9994, -0.3556, -0.2930, -0.3543, -0.4148,  0.3343,
         -0.3885, -1.0000,  0.3276, -0.2173,  0.1424, -0.1302,  0.1843, -0.4258,
         -0.9793, -0.3104,  0.3368,  0.0846, -0.5829, -0.6473,  0.4878, -0.2279,
          0.6974,  0.8775, -0.3410,  0.4726,  0.5304,  0.0258, -0.5915,  0.8673]],
       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)
model output last hidden layer  tensor([[[ 0.0287,  0.1711, -0.0128,  ..., -0.4202,  0.2021,  0.0992],
         [ 0.4537,  0.2695, -0.2365,  ..., -0.1391,  0.5434, -0.0229],
         [ 0.8245,  0.2420,  0.5221,  ...,  0.0926, -0.2089, -1.2964],
         ...,
         [-0.2026, -0.0621, -0.2736,  ...,  0.1625,  0.3685, -0.1595],
         [ 0.0400,  0.1936, -0.0795,  ...,  0.1241,  0.1658, -0.0950],
         [ 0.1787,  0.1993,  0.0787,  ...,  0.0352, -0.0963,  0.0263]]],
       grad_fn=<NativeLayerNormBackward0>) torch.Size([1, 16, 768]) 3